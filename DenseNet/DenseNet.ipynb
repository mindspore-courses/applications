{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066e8908",
   "metadata": {},
   "source": [
    "## 基于MindSpore框架的DenseNet案例实现\n",
    "\n",
    "### 1 模型简介\n",
    "\n",
    "DenseNet模型于2017年在论文《Densely Connected Convolutional Networks》中被提出。DenseNet通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。本篇文章首先介绍DenseNet的原理以及网路架构，然后讲解DenseNet在MindSpore上的实现。\n",
    "\n",
    "#### 1.1 模型结构\n",
    "相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图1为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在通道维度上连接在一起，并作为下一层的输入。对于一个L层的网络，DenseNet共包含$ \\frac{L(L+1)}{2}$个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接连接来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://s1.ax1x.com/2022/09/24/xAeebR.jpg\" alt=\"image-2022092401\" style=\"zoom:75%;\" />\n",
    "    <br>\n",
    "    <div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图1 一个5层的密集块，增长率为k=4。每一层都将所有前面的特征图作为输入。</div>\n",
    "</center>\n",
    "\n",
    "传统的网络在L层的输出为：\n",
    "$$\n",
    "x_{l}=H_{l}(x_{l−1})\n",
    "$$\n",
    "\n",
    "而对于ResNet，增加了来自上一层输入的identity函数：\n",
    "\n",
    "$$\n",
    "x_{l} = H_{l}(x_{l−1})+x_{l-1}\n",
    "$$\n",
    "\n",
    "在DenseNet中，会连接前面所有层作为输入：\n",
    "$$\n",
    "x_{l}=H_{l}([x_{0},x_{1},...,x_{l−1}])\n",
    "$$\n",
    "\n",
    "其中，上面的$H_{l}( ⋅ )$代表是非线性转化函数（non-liear transformation），它是一个组合操作，其可能包括一系列的BN(Batch Normalization)，ReLU，Pooling及Conv操作。注意这里l层与 $l−1$层之间可能实际上包含多个卷积层。\n",
    "\n",
    "CNN网络一般要经过Pooling或者stride>1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。图2给出了具有三个密集块的深度 DenseNet，各个DenseBlock之间通过Transition连接在一起。\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://s1.ax1x.com/2022/09/24/xAu3xU.jpg\" alt=\"image-2022092402\" style=\"zoom:75%;\" />\n",
    "    <br>\n",
    "    <div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">图2 具有三个密集块的深度 DenseNet。 两个相邻块之间的层称为过渡层，并通过卷积和池化改变特征图大小。</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "#### 1.2 DenseNet优点\n",
    "\n",
    "a) 相比ResNet拥有更少的参数数量；\n",
    "\n",
    "b) 旁路加强了特征的重用；\n",
    "\n",
    "c) 网络更易于训练,并具有一定的正则效果；\n",
    "\n",
    "d) 缓解了gradient vanishing和model degradation的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ed8dd",
   "metadata": {},
   "source": [
    "## 2. 案例实现\n",
    "\n",
    "### 2.1 数据集下载与准备\n",
    "+ Mini-ImageNet：https://drive.google.com/drive/folders/17a09kkqVivZQFggCw9I_YboJ23tcexNM\n",
    "\n",
    "+ 下载的数据集根目录dataset/下有train、val、test文件夹\n",
    "+ 但由于该数据集用于元学习和小样本学习，数据集的划分不是从每个类别中进行采样的，所以需要进一步对下载的数据集重新进行train、val、test的划分，使train、val、test都包含全部的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6704b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil\n",
    "\n",
    "# 先把上述链接中下载好的val、test目录下的所有文件夹都剪切至train目录，完成合并\n",
    "# 如下代码是使train、val、test都含有全部100个分类，并针对每个分类按照6:2:2的比例划分\n",
    "\n",
    "def moveFile(fileDir, tarDir, rate):\n",
    "    pathDir = os.listdir(fileDir)  # 取图片的原始路径\n",
    "    filenumber = len(pathDir)\n",
    "    if filenumber > 360: # 重新划分好的train目录下每个分类的图片数量=360，此行是为了防止重复进行划分\n",
    "        rate = rate  # 自定义抽取图片的比例，比方说100张抽10张，那就是0.1\n",
    "        picknumber = int(filenumber * rate)  # 按照rate比例从文件夹中取一定数量图片\n",
    "        sample = random.sample(pathDir, picknumber)  # 随机选取picknumber数量的样本图片\n",
    "        for name in sample:\n",
    "            shutil.move(fileDir + '/' + name, tarDir + '/' + name)\n",
    "\n",
    "data_dir = \"dataset/train\"\n",
    "\n",
    "for classes_num in os.listdir(data_dir):\n",
    "    src_dir = os.path.join(\"./dataset/train/\",classes_num)\n",
    "    dst_dir_val = os.path.join(\"./dataset/val/\", classes_num)\n",
    "    dst_dir_test = os.path.join(\"./dataset/test/\", classes_num)\n",
    "    \n",
    "    if not os.path.exists(dst_dir_val):\n",
    "        os.mkdir(dst_dir_val)\n",
    "    \n",
    "    if not os.path.exists(dst_dir_test):\n",
    "        os.mkdir(dst_dir_test)\n",
    "    \n",
    "    moveFile(src_dir, dst_dir_val, 0.2)\n",
    "    moveFile(src_dir, dst_dir_test, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2796e",
   "metadata": {},
   "source": [
    "+ 重新划分后，train、val、test每个文件夹下目录结构如下：\n",
    "```\n",
    ".\n",
    "└── train/val/test \n",
    "     ├── class1\n",
    "     │    ├── 000000000001.jpg\n",
    "     │    ├── 000000000002.jpg\n",
    "     │    ├── ...\n",
    "     ├── class2\n",
    "     │    ├── 000000000001.jpg\n",
    "     │    ├── 000000000002.jpg\n",
    "     │    ├── ...\n",
    "     ├── class3\n",
    "     │    ├── 000000000001.jpg\n",
    "     │    ├── 000000000002.jpg\n",
    "     │    ├── ...\n",
    "     ├── class100\n",
    "          ├── 000000000001.jpg\n",
    "          ├── 000000000002.jpg\n",
    "          ├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700e662",
   "metadata": {},
   "source": [
    "### 2.2 数据集创建\n",
    "在创建数据集时使用了mindspore框架提供的ImageFolderDataset，该方法可以从从树状结构的文件目录中读取图片构建源数据集，也就是在2.1中处理好的train/val/test目录，且这3个目录下都包含100个分类目录，同一个分类目录的所有图片都会被分配相同的label，由此即可返回生成好的两列数据集：\\[image, label\\]。此外数据集创建时还进行了包括数据增强，归一化等transfrom定义，之后通过mindspore的map方法来分别完成样本和label的transform映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.dataset as de\n",
    "import mindspore.dataset.vision as vision_C\n",
    "import mindspore.dataset.transforms as normal_C\n",
    "from mindspore.dataset.vision import Inter\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def classification_dataset(data_dir, image_size, per_batch_size, max_epoch, rank,group_size,\n",
    "                           mode='train', input_mode='folder', root='', num_parallel_workers=None,\n",
    "                           shuffle=None, sampler=None, class_indexing=None, drop_remainder=True,\n",
    "                           transform=None, target_transform=None):\n",
    "   \n",
    "\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "    if transform is None:\n",
    "        if mode == 'train':\n",
    "            transform_img = [\n",
    "                vision_C.Decode(),\n",
    "                vision_C.Resize(image_size, interpolation=Inter.NEAREST),\n",
    "                vision_C.RandomHorizontalFlip(prob=0.5),\n",
    "                vision_C.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                vision_C.Normalize(mean=mean, std=std),\n",
    "                vision_C.HWC2CHW()]\n",
    "        else:\n",
    "            transform_img = [\n",
    "                vision_C.Decode(),\n",
    "                vision_C.Resize(image_size, interpolation=Inter.NEAREST),\n",
    "                vision_C.Normalize(mean=mean, std=std),\n",
    "                vision_C.HWC2CHW()]\n",
    "    else:\n",
    "        transform_img = transform\n",
    "\n",
    "    if target_transform is None:\n",
    "        transform_label = [normal_C.TypeCast(mstype.int32)]\n",
    "    else:\n",
    "        transform_label = target_transform\n",
    "\n",
    "    de_dataset = de.ImageFolderDataset(data_dir, num_parallel_workers=num_parallel_workers,\n",
    "                                       shuffle=shuffle, sampler=sampler, class_indexing=class_indexing,\n",
    "                                       num_shards=group_size, shard_id=rank)\n",
    "\n",
    "    de_dataset = de_dataset.map(input_columns=\"image\", num_parallel_workers=8, operations=transform_img)\n",
    "    de_dataset = de_dataset.map(input_columns=\"label\", num_parallel_workers=8, operations=transform_label)\n",
    "\n",
    "    columns_to_project = [\"image\", \"label\"]\n",
    "    de_dataset = de_dataset.project(columns=columns_to_project)\n",
    "    de_dataset = de_dataset.batch(per_batch_size, drop_remainder=drop_remainder)\n",
    "    de_dataset = de_dataset.repeat(1)\n",
    "\n",
    "    return de_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fcd70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (4, 3, 224, 224) Float32 --- Shape of label [N, C, H, W]: (4,) Int32\n",
      "Shape of image [N, C, H, W]: (4, 3, 224, 224) Float32 --- Shape of label [N, C, H, W]: (4,) Int32\n",
      "Shape of image [N, C, H, W]: (4, 3, 224, 224) Float32 --- Shape of label [N, C, H, W]: (4,) Int32\n",
      "Shape of image [N, C, H, W]: (4, 3, 224, 224) Float32 --- Shape of label [N, C, H, W]: (4,) Int32\n",
      "Shape of image [N, C, H, W]: (4, 3, 224, 224) Float32 --- Shape of label [N, C, H, W]: (4,) Int32\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_dir = \"dataset/train\"\n",
    "    train_dataset = classification_dataset(data_dir, image_size=[224, 224],per_batch_size=4, max_epoch=20, rank=0, group_size=1)\n",
    "    for item, (image, label) in enumerate(train_dataset):\n",
    "        if item < 5:\n",
    "            print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\",'---',f\"Shape of label [N, C, H, W]: {label.shape} {label.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274791d8",
   "metadata": {},
   "source": [
    "### 2.3 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fda6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "import mindspore.nn as nn\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common import initializer as init\n",
    "__all__ = [\"DenseNet121\", \"DenseNet100\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb66c79",
   "metadata": {},
   "source": [
    "+ 定义Kaiming初始化函数\n",
    "\n",
    "本案例在构建Densenet121网络时，需要计算的Kaiming初始化函数，根据Kaiming Normal计算公式：$ \\frac{gain}{\\sqrt{fan}}$，具体流程首先时定义了KamingInit类，并通过_calculate_gain函数计算公式分子gain，再通过定义继承了KamingInit类的KamingNormal类，完成分母fan的计算，完成针对数组的Kaiming初始化函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0921937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n",
    "\n",
    "    fan_in, fan_out = _calculate_in_and_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def _calculate_gain(nonlinearity, param=None):\n",
    "    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    if nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    if nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    if nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(\"negative_slope {} not a valid number\".format(param))\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "\n",
    "    raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n",
    "    \n",
    "    \n",
    "def _assignment(arr, num):\n",
    "    if arr.shape == ():\n",
    "        arr = arr.reshape((1))\n",
    "        arr[:] = num\n",
    "        arr = arr.reshape(())\n",
    "    else:\n",
    "        if isinstance(num, np.ndarray):\n",
    "            arr[:] = num[:]\n",
    "        else:\n",
    "            arr[:] = num\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _calculate_in_and_out(arr):\n",
    "    dim = len(arr.shape)\n",
    "    if dim < 2:\n",
    "        raise ValueError(\"If initialize data with xavier uniform, the dimension of data must greater than 1.\")\n",
    "\n",
    "    n_in = arr.shape[1]\n",
    "    n_out = arr.shape[0]\n",
    "\n",
    "    if dim > 2:\n",
    "        counter = reduce(lambda x, y: x * y, arr.shape[2:])\n",
    "        n_in *= counter\n",
    "        n_out *= counter\n",
    "    return n_in, n_out\n",
    "\n",
    "\n",
    "class KaimingInit(init.Initializer):\n",
    "    def __init__(self, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
    "        super(KaimingInit, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.gain = _calculate_gain(nonlinearity, a)\n",
    "\n",
    "    def _initialize(self, arr):\n",
    "        pass\n",
    "\n",
    "class KaimingUniform(KaimingInit):\n",
    "    def _initialize(self, arr):\n",
    "        fan = _select_fan(arr, self.mode)\n",
    "        bound = math.sqrt(3.0) * self.gain / math.sqrt(fan)\n",
    "        data = np.random.uniform(-bound, bound, arr.shape)\n",
    "\n",
    "        _assignment(arr, data)\n",
    "\n",
    "class KaimingNormal(KaimingInit):\n",
    "    def _initialize(self, arr):\n",
    "        fan = _select_fan(arr, self.mode)\n",
    "        std = self.gain / math.sqrt(fan)\n",
    "        data = np.random.normal(0, std, arr.shape)\n",
    "\n",
    "        _assignment(arr, data)\n",
    "\n",
    "\n",
    "def default_recurisive_init(custom_cell):\n",
    "    for _, cell in custom_cell.cells_and_names():\n",
    "        if isinstance(cell, nn.Conv2d):\n",
    "            cell.weight.set_data(init.initializer(KaimingUniform(a=math.sqrt(5)), cell.weight.shape, cell.weight.dtype))\n",
    "            if cell.bias is not None:\n",
    "                fan_in, _ = _calculate_in_and_out(cell.weight.asnumpy())\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                cell.bias.set_data(Tensor(np.random.uniform(-bound, bound, cell.bias.shape), cell.bias.dtype))\n",
    "        elif isinstance(cell, nn.Dense):\n",
    "            cell.weight.set_data(init.initializer(KaimingUniform(a=math.sqrt(5)), cell.weight.shape, cell.weight.dtype))\n",
    "            if cell.bias is not None:\n",
    "                fan_in, _ = _calculate_in_and_out(cell.weight.asnumpy())\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                cell.bias.set_data(Tensor(np.random.uniform(-bound, bound, cell.bias.shape), cell.bias.dtype))\n",
    "        elif isinstance(cell, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10deb9",
   "metadata": {},
   "source": [
    "+ 构建DenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dc91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPooling(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPooling, self).__init__()\n",
    "        self.mean = P.ReduceMean(True)\n",
    "        self.shape = P.Shape()\n",
    "        self.reshape = P.Reshape()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.mean(x, (2, 3))\n",
    "        b, c, _, _ = self.shape(x)\n",
    "        x = self.reshape(x, (b, c))\n",
    "        return x\n",
    "\n",
    "class CommonHead(nn.Cell):\n",
    "    def __init__(self, num_classes, out_channels):\n",
    "        super(CommonHead, self).__init__()\n",
    "        self.avgpool = GlobalAvgPooling()\n",
    "        self.fc = nn.Dense(out_channels, num_classes, has_bias=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def conv7x7(in_channels, out_channels, stride=1, padding=3, has_bias=False):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, has_bias=has_bias,\n",
    "                     padding=padding, pad_mode=\"pad\")\n",
    "\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, has_bias=False):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, has_bias=has_bias,\n",
    "                     padding=padding, pad_mode=\"pad\")\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels, stride=1, padding=0, has_bias=False):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, has_bias=has_bias,\n",
    "                     padding=padding, pad_mode=\"pad\")\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Cell):\n",
    "    \"\"\"\n",
    "    the dense layer, include 2 conv layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv1 = conv1x1(num_input_features, bn_size*growth_rate)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm2d(bn_size*growth_rate)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv2 = conv3x3(bn_size*growth_rate, growth_rate)\n",
    "\n",
    "        # nn.Dropout in MindSpore use keep_prob, diff from Pytorch\n",
    "        self.keep_prob = 1.0 - drop_rate\n",
    "        self.dropout = nn.Dropout(keep_prob=self.keep_prob)\n",
    "\n",
    "    def construct(self, features):\n",
    "        bottleneck = self.conv1(self.relu1(self.norm1(features)))\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck)))\n",
    "        if self.keep_prob < 1:\n",
    "            new_features = self.dropout(new_features)\n",
    "        return new_features\n",
    "\n",
    "class _DenseBlock(nn.Cell):\n",
    "    \"\"\"\n",
    "    the dense block\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        self.cell_list = nn.CellList()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            self.cell_list.append(layer)\n",
    "\n",
    "        self.concate = P.Concat(axis=1)\n",
    "\n",
    "    def construct(self, init_features):\n",
    "        features = init_features\n",
    "        for layer in self.cell_list:\n",
    "            new_features = layer(features)\n",
    "            features = self.concate((features, new_features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf88600",
   "metadata": {},
   "source": [
    "+ 构建Transition层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5878174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Transition(nn.Cell):\n",
    "    \"\"\"\n",
    "    the transition layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_features, num_output_features, avgpool=False):\n",
    "        super(_Transition, self).__init__()\n",
    "        if avgpool:\n",
    "            poollayer = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        else:\n",
    "            poollayer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.features = nn.SequentialCell(OrderedDict([\n",
    "            ('norm', nn.BatchNorm2d(num_input_features)),\n",
    "            ('relu', nn.ReLU()),\n",
    "            ('conv', conv1x1(num_input_features, num_output_features)),\n",
    "            ('pool', poollayer)\n",
    "        ]))\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e53d09",
   "metadata": {},
   "source": [
    "+ 构建DenseNet主干网络框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8591b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Densenet(nn.Cell):\n",
    "    \"\"\"\n",
    "    the densenet architecture\n",
    "    \"\"\"\n",
    "    __constants__ = ['features']\n",
    "\n",
    "    def __init__(self, growth_rate, block_config, num_init_features=None, bn_size=4, drop_rate=0):\n",
    "        super(Densenet, self).__init__()\n",
    "\n",
    "        layers = OrderedDict()\n",
    "        if num_init_features:\n",
    "            layers['conv0'] = conv7x7(3, num_init_features, stride=2, padding=3)\n",
    "            layers['norm0'] = nn.BatchNorm2d(num_init_features)\n",
    "            layers['relu0'] = nn.ReLU()\n",
    "            layers['pool0'] = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "            num_features = num_init_features\n",
    "        else:\n",
    "            layers['conv0'] = conv3x3(3, growth_rate*2, stride=1, padding=1)\n",
    "            layers['norm0'] = nn.BatchNorm2d(growth_rate*2)\n",
    "            layers['relu0'] = nn.ReLU()\n",
    "            num_features = growth_rate * 2\n",
    "\n",
    "        # Each denseblock\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            layers['denseblock%d'%(i+1)] = block\n",
    "            num_features = num_features + num_layers*growth_rate\n",
    "\n",
    "            if i != len(block_config)-1:\n",
    "                if num_init_features:\n",
    "                    trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2,\n",
    "                                        avgpool=False)\n",
    "                else:\n",
    "                    trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2,\n",
    "                                        avgpool=True)\n",
    "                layers['transition%d'%(i+1)] = trans\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        layers['norm5'] = nn.BatchNorm2d(num_features)\n",
    "        layers['relu5'] = nn.ReLU()\n",
    "\n",
    "        self.features = nn.SequentialCell(layers)\n",
    "        self.out_channels = num_features\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def get_out_channels(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    \n",
    "def _densenet121(**kwargs):\n",
    "    return Densenet(growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb4f0a",
   "metadata": {},
   "source": [
    "+ 构建DenseNet121网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b788be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Cell):\n",
    "    \"\"\"\n",
    "    the densenet121 architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, include_top=True):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.backbone = _densenet121()\n",
    "        out_channels = self.backbone.get_out_channels()\n",
    "        self.include_top = include_top\n",
    "        if self.include_top:\n",
    "            self.head = CommonHead(num_classes, out_channels)\n",
    "\n",
    "        default_recurisive_init(self)\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, nn.Conv2d):\n",
    "                cell.weight.set_data(init.initializer(KaimingNormal(a=math.sqrt(5), mode='fan_out',\n",
    "                                                                    nonlinearity='relu'),\n",
    "                                                      cell.weight.shape,\n",
    "                                                      cell.weight.dtype))\n",
    "            elif isinstance(cell, nn.BatchNorm2d):\n",
    "                cell.gamma.set_data(init.initializer('ones', cell.gamma.shape))\n",
    "                cell.beta.set_data(init.initializer('zeros', cell.beta.shape))\n",
    "            elif isinstance(cell, nn.Dense):\n",
    "                cell.bias.set_data(init.initializer('zeros', cell.bias.shape))\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if not self.include_top:\n",
    "            return x\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497b3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore.nn.optim import Momentum\n",
    "from mindspore.communication.management import init, get_rank, get_group_size\n",
    "from mindspore.train.callback import ModelCheckpoint\n",
    "from mindspore.train.callback import CheckpointConfig, Callback\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.loss_scale_manager import DynamicLossScaleManager, FixedLossScaleManager\n",
    "from mindspore import context\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.common import set_seed\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore.common import initializer as init\n",
    "from mindspore.nn.loss.loss import _Loss\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "#from utils.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "#from utils.crossentropy import CrossEntropy\n",
    "#from utils.optimizers import get_param_groups\n",
    "# set_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276532a",
   "metadata": {},
   "source": [
    "### 2.4 模型训练及评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af056d60",
   "metadata": {},
   "source": [
    "+ 定义学习率决策\n",
    "\n",
    "定义了CosineAnnealingLR学习率scheduler决策，以便于在后续的模型训练中使用，主要包含lr, T_max, eta_min等几种参数，其中lr就是设置好的初始学习率，T_max是cosine循环的最大次数，eta_min是在consine循环中最低的学习率，通过修改T_max与eta_min这两个参数可以直接调整训练过程中学习率的变化曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe5716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WarmUp():\n",
    "    def __init__(self, warmup_init_lr):\n",
    "        self.warmup_init_lr = warmup_init_lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Get learning rate during warmup\n",
    "        raise NotImplementedError\n",
    "\n",
    "class _LinearWarmUp(_WarmUp):\n",
    "    \"\"\"\n",
    "    linear warmup function\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, warmup_epochs, steps_per_epoch, warmup_init_lr=0):\n",
    "        self.base_lr = lr\n",
    "        self.warmup_init_lr = warmup_init_lr\n",
    "        self.warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
    "\n",
    "        super(_LinearWarmUp, self).__init__(warmup_init_lr)\n",
    "\n",
    "    def get_warmup_steps(self):\n",
    "        return self.warmup_steps\n",
    "\n",
    "    def get_lr(self, current_step):\n",
    "        lr_inc = (float(self.base_lr) - float(self.warmup_init_lr)) / float(self.warmup_steps)\n",
    "        lr = float(self.warmup_init_lr) + lr_inc * current_step\n",
    "        return lr\n",
    "\n",
    "class _LRScheduler():\n",
    "\n",
    "    def __init__(self, lr, max_epoch, steps_per_epoch):\n",
    "        self.base_lr = lr\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.total_steps = int(max_epoch * steps_per_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Compute learning rate using chainable form of the scheduler\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "        \n",
    "class CosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, lr, T_max, steps_per_epoch, max_epoch, warmup_epochs=0, eta_min=0):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.warmup = _LinearWarmUp(lr, warmup_epochs, steps_per_epoch)\n",
    "        super(CosineAnnealingLR, self).__init__(lr, max_epoch, steps_per_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        warmup_steps = self.warmup.get_warmup_steps()\n",
    "\n",
    "        lr_each_step = []\n",
    "        current_lr = self.base_lr\n",
    "        for i in range(self.total_steps):\n",
    "            if i < warmup_steps:\n",
    "                lr = self.warmup.get_lr(i+1)\n",
    "            else:\n",
    "                cur_ep = i // self.steps_per_epoch\n",
    "                if i % self.steps_per_epoch == 0 and i > 0:\n",
    "                    current_lr = self.eta_min + \\\n",
    "                                 (self.base_lr - self.eta_min) * (1. + math.cos(math.pi*cur_ep / self.T_max)) / 2\n",
    "\n",
    "                lr = current_lr\n",
    "\n",
    "            lr_each_step.append(lr)\n",
    "\n",
    "        return np.array(lr_each_step).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854f7da",
   "metadata": {},
   "source": [
    "+ 定义交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79559722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(_Loss):\n",
    "    \"\"\"\n",
    "    loss function CrossEntropy\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth_factor=0., num_classes=1000):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.onehot = P.OneHot()\n",
    "        self.on_value = Tensor(1.0 - smooth_factor, mstype.float32)\n",
    "        self.off_value = Tensor(1.0 * smooth_factor / (num_classes -1), mstype.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits()\n",
    "        self.mean = P.ReduceMean(False)\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        one_hot_label = self.onehot(label,\n",
    "                                    F.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss = self.ce(logit, one_hot_label)\n",
    "        loss = self.mean(loss, 0)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441403f",
   "metadata": {},
   "source": [
    "+ 模型训练\n",
    "\n",
    "在模型训练时，首先是设置模型训练的epoch次数，再通过2.1节中自定义的classification_dataset方法创建了训练集，其中训练集batch_size大小为4；损失函数使用CrossEntropy，优化器使用Momentum，并设置初始学习率为0.1。回调函数方面使用了自定义的ProgressMonitor来监控训练过程中每个epoch和每个step结束后，损失值Loss的变化情况，并在训练结束后保存当前最优模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7592f24-b209-4ed5-89f4-f9f66d5ded04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 3]\n",
      "Train Loss=4.6541, top1_correct=391, top5_correct=1897, tot=35840, Top1_acc=1.09%, Top5_acc=5.29%\n",
      "Val Loss=4.6363 top1_correct=136, top5_correct=634, tot=11776, Top1_acc=1.15%, Top5_acc=5.38%\n",
      "Acc improved from 0.0000 to 0.0115\n",
      "saving best checkpoint at: checkpoint/densenet.ckpt \n",
      "Epoch [2 / 3]\n",
      "Train Loss=4.6438, top1_correct=362, top5_correct=1791, tot=35840, Top1_acc=1.01%, Top5_acc=5.00%\n",
      "Val Loss=4.6475 top1_correct=137, top5_correct=589, tot=11776, Top1_acc=1.16%, Top5_acc=5.00%\n",
      "Acc improved from 0.0115 to 0.0116\n",
      "saving best checkpoint at: checkpoint/densenet.ckpt \n",
      "Epoch [3 / 3]\n",
      "Train Loss=4.6490, top1_correct=321, top5_correct=1820, tot=35840, Top1_acc=0.90%, Top5_acc=5.08%\n",
      "Val Loss=4.6559 top1_correct=96, top5_correct=590, tot=11776, Top1_acc=0.82%, Top5_acc=5.01%\n",
      "Acc did not improve from 0.0116 \n",
      "-------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from mindspore import ops\n",
    "from mindspore import ms_function\n",
    "from mindspore import context\n",
    "import mindspore\n",
    "def get_param_groups(network):\n",
    "    \"\"\"\n",
    "    get parameter groups\n",
    "    \"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for x in network.trainable_params():\n",
    "        parameter_name = x.name\n",
    "        if parameter_name.endswith('.bias'):\n",
    "            # all bias not using weight decay\n",
    "            no_decay_params.append(x)\n",
    "        elif parameter_name.endswith('.gamma'):\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include BN\n",
    "            no_decay_params.append(x)\n",
    "        elif parameter_name.endswith('.beta'):\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include BN\n",
    "            no_decay_params.append(x)\n",
    "        else:\n",
    "            decay_params.append(x)\n",
    "\n",
    "    return [{'params': no_decay_params, 'weight_decay': 0.0}, {'params': decay_params}]\n",
    "\n",
    "def get_top5_acc(top5_arg, gt_class):\n",
    "    sub_count = 0\n",
    "    for top5, gt in zip(top5_arg, gt_class):\n",
    "        if gt in top5:\n",
    "            sub_count += 1\n",
    "    return sub_count\n",
    "\n",
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    # Define forward function\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    # Get gradient function\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "    # Define function of one-step training\n",
    "    # @ms_function\n",
    "    def train_step(data, label):\n",
    "        (loss, logits), grads = grad_fn(data, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss, logits\n",
    "\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train(True)\n",
    "    train_loss = 0\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    img_tot = 0\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss, logits = train_step(data, label)\n",
    "\n",
    "        logits = logits.asnumpy()\n",
    "        label = label.asnumpy()\n",
    "        top1_output = np.argmax(logits, (-1))\n",
    "        top5_output = np.argsort(logits)[:, -5:]\n",
    "\n",
    "        t1_correct = np.equal(top1_output, label).sum()\n",
    "        top1_correct += t1_correct\n",
    "        top5_correct += get_top5_acc(top5_output, label)\n",
    "        img_tot += label.shape[0]\n",
    "        train_loss += loss.asnumpy()\n",
    "\n",
    "    train_loss /= size\n",
    "    acc1 = 100.0 * top1_correct / img_tot\n",
    "    acc5 = 100.0 * top5_correct / img_tot\n",
    "    print('Train Loss={:.4f}, top1_correct={}, top5_correct={}, tot={}, Top1_acc={:.2f}%, Top5_acc={:.2f}%'.format(train_loss, top1_correct, top5_correct, img_tot, acc1, acc5))\n",
    "\n",
    "def val(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    val_loss = 0\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    img_tot = 0\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        logits = model(data)\n",
    "        val_loss += loss_fn(logits, label).asnumpy()\n",
    "        logits = logits.asnumpy()\n",
    "        label = label.asnumpy()\n",
    "        top1_output = np.argmax(logits, (-1))\n",
    "        top5_output = np.argsort(logits)[:, -5:]\n",
    "\n",
    "        t1_correct = np.equal(top1_output, label).sum()\n",
    "        top1_correct += t1_correct\n",
    "        top5_correct += get_top5_acc(top5_output, label)\n",
    "        img_tot += label.shape[0]\n",
    "        \n",
    "    val_loss /= size\n",
    "    acc1 = 100.0 * top1_correct / img_tot\n",
    "    acc5 = 100.0 * top5_correct / img_tot\n",
    "    print('Val Loss={:.4f} top1_correct={}, top5_correct={}, tot={}, Top1_acc={:.2f}%, Top5_acc={:.2f}%'.format(val_loss, top1_correct, top5_correct, img_tot, acc1, acc5))\n",
    "    return top1_correct / img_tot\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target='Ascend')\n",
    "train_dataset = classification_dataset(\"dataset/train\", image_size=[168, 168],per_batch_size=512, max_epoch=20, rank=0, group_size=1, mode = 'train')\n",
    "val_dataset = classification_dataset(\"dataset/val\", image_size=[168, 168],per_batch_size=512, max_epoch=20, rank=0, group_size=1, mode = 'val')\n",
    "steps_per_epoch = train_dataset.get_dataset_size()\n",
    "\n",
    "epochs = 3\n",
    "net = DenseNet121(100)\n",
    "criterion = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "# criterion = CrossEntropy(smooth_factor=0.0, num_classes=100)\n",
    "lr_scheduler = CosineAnnealingLR(lr=0.0001,T_max=120,steps_per_epoch=steps_per_epoch,max_epoch=epochs,warmup_epochs=0,eta_min=0)\n",
    "lr_schedule = lr_scheduler.get_lr()\n",
    "opt = Momentum(params=get_param_groups(net),learning_rate=Tensor(lr_schedule),momentum=0.9,weight_decay=0.0001,loss_scale=1024)\n",
    "# opt = nn.SGD(params=net.trainable_params(),learning_rate=0.001)\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch+1} / {epochs}]\")\n",
    "    train(net, train_dataset, criterion, opt)\n",
    "    checkpoint_best = val(net, val_dataset,criterion)\n",
    "    if checkpoint_best > best_acc:\n",
    "        print('Acc improved from %0.4f to %0.4f' % (best_acc, checkpoint_best))\n",
    "        best_acc = checkpoint_best\n",
    "        mindspore.save_checkpoint(net, 'checkpoint/densenet.ckpt')\n",
    "        print(\"saving best checkpoint at: {} \".format('checkpoint/densenet.ckpt'))\n",
    "    else:\n",
    "        print('Acc did not improve from %0.4f' % (best_acc),\"\\n-------------------------------\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f42c9",
   "metadata": {},
   "source": [
    "### 2.5 模型效果评估\n",
    "\n",
    "模型效果评估方面首先是通过2.1节中自定义的classification_dataset方法创建了验证集，之后通过mindspore提供的load_checkpoint方法加载模型参数，再将测试集输入模型完成预测，并分别计算预测类别的Top-1和Top-5准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13647fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "from mindspore.communication.management import init, get_rank, get_group_size, release\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.common import initializer as init\n",
    "from mindspore import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "205acf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    data_dir = \"dataset/val\"\n",
    "    dataset = classification_dataset(\"dataset/test\", image_size=[168, 168], per_batch_size=512, max_epoch=20, rank=0, group_size=1, mode = 'val')\n",
    "    \n",
    "    network = DenseNet121(100)\n",
    "    load_checkpoint(\"checkpoint/densenet.ckpt\", net=network)\n",
    "    \n",
    "    # network.add_flags_recursive(fp16=True)\n",
    "\n",
    "    img_tot = 0\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    network.set_train(False)\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        logits = network(data)\n",
    "        logits = logits.asnumpy()\n",
    "        label = label.asnumpy()\n",
    "        top1_output = np.argmax(logits, (-1))\n",
    "        top5_output = np.argsort(logits)[:, -5:]\n",
    "\n",
    "        t1_correct = np.equal(top1_output, label).sum()\n",
    "        top1_correct += t1_correct\n",
    "        top5_correct += get_top5_acc(top5_output, label)\n",
    "        img_tot += label.shape[0]\n",
    "        \n",
    "    acc1 = 100.0 * top1_correct / img_tot\n",
    "    acc5 = 100.0 * top5_correct / img_tot\n",
    "    print('Test top1_correct={}, top5_correct={}, tot={}, Top1_acc={:.2f}%, Top5_acc={:.2f}%'.format(top1_correct, top5_correct, img_tot, acc1, acc5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9800b65-65c6-4990-a1ab-892db1cd0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test top1_correct=129, top5_correct=592, tot=11776, Top1_acc=1.10%, Top5_acc=5.03%\n"
     ]
    }
   ],
   "source": [
    "evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d0ebc",
   "metadata": {},
   "source": [
    "## 3 总结\n",
    "\n",
    "本案例基于MindSpore框架针对mini-ImageNet数据集，首先在对原始数据集进行预处理后完成了数据读取与数据集创建，之后通过框架构建了Densenet121模型并自定义了回调函数，并在模型训练后，进行了分类索引与数据集提供的分类编号、分类名称的转换，顺利利用模型完成了评估和预测。通过此案例进一步加深了对Densenet模型结构和特性的理解，并通过使用MindSpore框架高效地完成了整个案例实现流程，加深了对框架提供的API的理解与掌握。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
